## 一、特征工程

### 特征归一化

- 将所有特征统一到一个大致相同的数值区间内， 常用归一化方法：
  1. 线性函数归一化（min-max），结果映射到[0，1]范围内
  2. 零均值归一化（Z-score），结果映射到均值为0， 标准差为1的分布上
- 数据归一化可以加速梯度下降收敛速度

>实际应用中， 通过梯度下求解的模型通常是需要归一化的， 包括线性回归、逻辑回归、支持向量机、神经网络模型等，但是对于决策树并不适用， 归一化并不会改变样本在特征x上的信息增益

### 类别特征

- 序号编码：用于类别间具有大小关系的数据， 比如高、中、低， 映射为3/2/1，保留了大小信息
- one-hot编码：用于处理类别间不具有大小关系的特征，可以理解为坐标， 保证了不同类别的距离相同， 当类别较多时， 需要注意一些问题：
  - 使用稀疏向量来节省空间
  - 配合特征选择来降低维度
    - 在k近邻算法中， 高维空间下两点的距离很难得到有效的衡量
    - 逻辑回归模型中， 参数的数量会随着维度的增高而增加， 容易引起过拟合问题
    - 一般只有部分维度对分类或预测有帮助，可以考虑配合特征选择来降低维度
- 二进制编码， 利用二进制对ID进行哈希映射，1->001, 2->010, 3->011

### 组合特征

- 将一阶离散特征两两组合， 构成高阶特征， 提高复杂模型的拟合能力
- 当特征数很多，如m个物品， n个用户， 此时两两组合的规模就为m*n, 在处理这种高维组合特征时， 一般将用户和物品用k维的向量表示，k远小于m和n， 需要学习的参数规模就大大降低 

- 可通过**决策树**算法寻找组合特征

### 文本向量

- 词袋模型：将文本以单词为分割， 每篇文本表示为一个向量， 常用TF-IDF来计算权重。词袋模型将句子拆为单词， 失去了句子含义， 并不是一种好的解决方式
- N-Gram模型：将连续出现的n个词作为一个单独的特征放到向量中，同一个单词可能有多种词性的变化， 却具有相似的含义
- 主题模型：从文本中发现具有代表性的主题
- 词嵌入模型：将每个词都映射成低维空间上的一个稠密向量，如word2vec模型

### Word2Vec

- CBOW：根据上下文出现的词语预测当前词的生成概率
- skip-gram：根据当前词来预测上下文中各词的生成概率
- 由于softmax激活函数中存在归一化项的缘故， 迭代时需要对词汇表中的所有单词进行遍历， 使得迭代过程非常缓慢， 由此产生了两种改进方法
  - 分层采样 Hieararchical Softmax：
    - 哈夫曼树， 带权路径长度最短的二叉树
    - ![image-20201114232046049](https://github.com/goblincc/postImg/blob/master/machine/image-20201114232046049.png)
  - 负采样 Negative Sampling：采样算法保证频次越高的样本越容易被采样出来， 对于长度为1的线段，根据词的频率将其公平的分配给每个词语， 然后生成一个0到1的随机数， 看落到哪个区间， 就能采样到该区间的单词了



## 二、模型评估

### 准确率

- 分类正确的样本占总样本个数的比例
- 缺点：当负样本占99%时， 分类器把所有样本都预测为负样本也可以获得99%的准确率， 当不同类别的样本比例非常不均匀时， 占比大的类别非常影响准确率

### 精确率与召回率

- 精确率： 分类正确的正样本个数占分类器判定为正样本的样本个数比例
- 召回率：分类正确的正样本个数占真正的正样本个数的比例
- 提高精确率时， 分类器将样本预测为正样本更为严格 导致召回率降低， 可能漏掉一些正样本，实际推荐场景中可能导致推荐的物品精确率高，但是用户仍然找不到想要的物品（排序靠后的物品）
- 更好的评估方法：P-R曲线， F1score（精确率和召回率的调和平均值）、ROC曲线

### RMSE、MAPE

- 反应预测值跟真实值的偏离程度用来衡量回归模型的好坏

- 问题：在95%的时间内预测误差都小于1%， 但是RMSE整体指标却很差是什么原因？

  - 实际场景中存在一些偏离程度非常大的离群点， 及时离群点较少也能使RMSE指标变差

  - 解决方案：

    1. 如果是认为是噪声点，需要在数据预处理阶段把噪声点过滤掉

    2. MAPE（Mean Absolute Percent Error）平均绝对百分比指标评估 

       ![image-20201115000203118](https://github.com/goblincc/postImg/raw/master/machine/image-20201115000203118.png)

       相比RMSE， MAPE相当于把每个点的误差进行了归一化， 降低了个别离群点带来的绝对误差影响

       > 当真实数据中有0时， 存在分母为0的情况， 公式不可用

  ### ROC曲线

- 绘制方法一：
  - 模型的输出一般都是预测样本为正例的概率

  - 假设测试集中有20个样本， 样本按照预测概率从高到低排序

  - 指定一个阈值， 预测概率大于该阈值的样本会被判为正例， 小于该阈值的样本判为负例

  - 动态调整阈值， 从最高得分开始逐渐调整到最低得分， 每个截断点会对应一个FPR和TPR， 绘出ROC曲

    线

 - 绘制方法二：

   - 根据样本标签统计出正负样本数量，正样本数量为p, 负样本数量为N
   - 把横轴的刻度间隔设置为1/N, 纵轴的刻度间隔设置为1/P
   - 根据模型输出的预测概率对样本进行从高到低排序
   - 依次遍历样本， 遇到一个正样本就沿纵轴方向绘制一个刻度的间隔线，遇到负样本沿横轴绘制一个刻度的间隔线

- AUC计算： 沿ROC横轴做积分，得到ROC曲线下的面积， 也可以按公式计算：

  ![auc](https://github.com/goblincc/postImg/raw/master/machine/image-20201115103901069.png)

- ROC曲线相比P-R曲线有什么特点：
  - 当正负样本的分布发生变化时， ROC曲线的形状能够基本保持不变， 而P-R曲线的形状会发生剧烈的变化， 因此ROC曲线能更好的反应模型本身的好坏
  - 如果希望看到模型在特定数据集上的表现， P-R曲线则能够更直观的反应其性能

### 余弦距离

- 欧式距离体现数值上的绝对差异， 而余弦距离体现方向上的相对差异

### A/B测试

- 离线模型评估好了， 为什么还要做线上A/B测试
  - 离线评估无法还原线上的工程环境，如环境延迟、数据丢失等，离线评估的结果是理想工程环境下的结果
  - 离线模型往往评估的是ROC曲线，P-R曲线等改进， 而线上评估可能需要关注用户点击率、留存时长等
- 如何进行线上A/B测试
  - 用户分桶
  - 样本的独立性和采样方式的无偏性， 确保同一个用户每次只分到同一个桶中

### 评估方法

- Holdout检验：原始样本划分为训练集和验证集两部分，如70%用于模型训练， 30%用于模型验证， 包括绘制ROC曲线，计算精确率和召回率等。缺点是评估指标与原始分组有关，具有随机性

- 交叉验证：k-fold交叉验证， 把K次评估指标的平均值作为最终的评估指标， 缺点是计算时间开销大

- 自助法： 对于总数为n的样本集合， 进行n次有放回的随机抽样，得到大小为n的训练集， n次采样过程中， 有的样本会被重复采样， 有的没有被抽出过 将没有被抽出的样本作为验证集，进行模型验证

  > n个样本进行n次自主采样，当n趋于无穷大时， 最终将有1/e = 36.8%的样本从未被采样过

### 过拟合与欠拟合

- 降低过拟合的方法：
  - 获得更多的训练数据， 更多的样本能够让模型学习到更多有效的特征， 减小噪声的影响
  - 降低模型的复杂度， 在数据较少时， 模型过于复杂是产生过拟合的主要因素， 如减少树的深度
  - 正则化方法，给模型的参数加上一定的正则约束
  - 集成学习方法， 把多个模型集成在一起， 降低单一模型过拟合方法， 如bagging
- 降低欠拟合的方法：
  - 添加新特征， 特征不足或者现有特征与样本标签相关性不强时， 容易出现欠拟合
  - 增加模型的复杂度
  - 减小正则化系数

### 偏差与方差

- 偏差：预测和真实结果的偏离程度， 刻画算法本身的拟合能力
- 方差：训练集的变动所导致的学习性能的变化，模型对于给定值的输出稳定性

> 偏差小， 方差大称为过拟合， 偏差大， 方差小称为欠拟合

## 三、经典算法

### 逻辑回归

### 决策树

- ID3: 最大信息增益  数据集经验熵 - 特征A对于数据集的经验条件熵
  - 信息增益反映的是给定条件以后不确定性减少的程度， 特征取值越多就意味着确定性越高，也就是条件熵越小， 信息增益越大，因此ID3会倾向于取值较多的特征
  - 假如引入特征DNA， 每个人的DNA都不同， 那ID3按DNA特征进行划分一定最优（条件熵为0），但这种泛化能力非常弱
- C4.5：最大信息增益比
  - 通过引入信息增益比一定程度上对取值比较多的特征进行惩罚， 避免ID3出现过拟合的特性提升决策树的泛化能力， 实际上是对ID3的一种优化
- CART：最大基尼指数 在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类

> - ID3只能处理离散型变量， 而C4.5和CART都可以处理连续型变量
> - ID3和C4.5只能用于分类任务， 而CART可用于分类也可用于回归
> - C4.5在处理连续型变量时， 通过对数据排序后找到类别不同的分割线作为切分点， 根据切分点把连续属性转换为布尔型， 从而将连续型变量转换为多个取值区间的离散型变量
> - CART由于构建时每次都会对特征进行二值划分，可以很好的适用于连续变量

- 决策树剪枝

  - 预剪枝： 

    1. 当树到达一定深度的时候， 停止树的生长

    2. 当到达当前节点的样本数量小于某个阈值的时候， 停止树的生长

    3. 计算每次分裂对测试集准确度的提升， 当小于某个阈值时， 不再继续扩展

       >效率高， 适合解决大规模问题， 但是参数难确定， 存在欠拟合的风险

  - 后剪枝

    1. 让算法生成一棵完全生长的决策树， 从最底层向上计算是否剪枝

    2. 将子树删除， 用一个叶子节点替代

    3. 剪枝过后如果在测试集上准确率提升， 则进行剪枝

       > 可以得到泛化能力强的决策树， 但时间开销会更大

### SVM、LR、决策树的对比

- SVM支持核函数、可处理非线性问题
- LR模型简单、训练速度快，适合处理线性问题
- 决策树容易过拟合，需要进行剪枝损失函数

## 四、降维

### PCA降维

- 线性、无监督、全局的降维算法

- 求解方法：
  - 对样本数据进行中心化处理
  - 求样本协方差矩阵
  - 对协方差矩阵进行特征值分解， 将特征值从大到小排列
  - 取特征值前d大对应的特征向量，将n维样本映射到d维

LDA

- Linear Discriminant Analysis 线性判别分析， 是一种有监督的学习算法
- 最大化类间距离和最小化类距离
- 善于对类别信息的数据进行降维处理

## 五、无监督学习

### K均值聚类

- 缺点：受初值和离群点的影响， 结果不稳定， 通常是局部最优解
- 调优：
  - 数据归一化和离群点处理
  - 可采用手肘法合理选择k值

### 高斯混合模型

- GMM (Gaussian Mixed Model) 使用EM算法进行迭代计算的聚类算法
- 当数据有多个类， 可以假设不同类各自服从不同的高斯分布， 由此得到的聚类算法成为高斯混合模型
- 相比K均值聚类， 都需要制定K值， 都使用EM算法求解， **高斯混合模型还可以给出一个样本属于某一类的概率是多少， 不仅仅可以用于聚类， 还可以用于概率密度的估计， 并且可以用于生成新的样本点**

### 聚类算法的评估

- 观察聚类误差是否随类别数量的增加而单调变化
- 观察聚类误差对实际聚类结果的影响
- 观察近邻数据簇的聚类准确性
- 观察数据密度具有较大差异的数据簇的聚类效果
- 样本数量具有较大差异的数据簇的聚类效果

## 六、优化算法

### 损失函数

- 有监督学习的损失函数有哪些

### 优化问题

- 哪些是凸优化问题， 哪些不是凸优化问题

### 梯度下降

- 经典的梯度下降算法存在什么问题， 如何改进
- 随机梯度下降法的加速

### 正则化

- L1正则化

- L2正则化缓解过拟合

  - 控制参数模的大小在一个范围内，模型对于微小输入扰动的反馈不会过大，会push模型曲线尽可能平滑，对应一个没那么复杂的模型

  - 模型对于微小扰动的反馈差异大实际就是一个过拟合的表现。很多时候对应着原始数据中的outlier，这也说明模型过于强调记住每一个sample，导致了过拟合

    

## 七、采样

### 常见采样方法	

- 逆变换采样、拒绝采样、重要性采样

### 高斯分布的采样

### 马尔科夫蒙特卡洛采样 MCMC

### 贝叶斯网络的采样

### 不均衡样本集的重采样

## 八、集成学习

### 集成学习的种类

- 集成学习分哪几种， 有何异同

### 集成学习的步骤

### 基分类器

- 常用基分类器
- 从减小方差和偏差的角度解释Boosting和Bagging的原理

### 梯度提升决策树的基本原理

- GBDT的基本原理
- 梯度提升和梯度下降的区别和联系
- GBDT的优点和局限性
  - 优点
    - 预测阶段的计算速度快， 树与树之间可并行化计算
    - 在分布稠密的数据集上， 泛化能力和表达能力都很好
    - 能够自动发现特征间的高阶关系， 不需要对数据进行归一化等预处理
  - 局限性
    - GBDT在高维稀疏的数据集上， 表现不如支持向量机
    - 训练过程需要串行训练， 只能在决策树内部采用一些局部并行的手段提高运行速度

### XGBoost与GBDT的联系和区别

- GBDT是机器学习算法， XGBOOST是该算法的工程实现
- 在使用CART作为基分类器时， XGBoost显示地加入了正则项来控制模型的复杂度， 有利于防止过拟合， 提高模型的泛化能力
- GBDT在模型训练时指使用了代价函数的一阶导数信息， XGBoost对代价函数进行二阶泰勒展开， 可以同时使用一阶和二阶导数
- GBDT采用CART作为基分类器， XGBoost支持多种类型的基分类器
- GBDT在每轮迭代时使用全部的数据， XGBoost采用了与随机森林类似的策略， 支持对数据采样
- GBDT没有设计对缺失值进行处理， XGBoost能够自动学习缺失值的处理策略
